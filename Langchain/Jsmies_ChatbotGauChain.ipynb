{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf chromadb langchain tiktoken openai streamlit python-dotenv unstructured pinecone-client sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP8LXQZPVQMf",
        "outputId": "a2a7fe81-c0e8-428b-e845-7bc5543dc4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-3.15.4-py3-none-any.whl (272 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.3/272.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.4.8-py3-none-any.whl (418 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.3/418.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.0.278-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit\n",
            "  Downloading streamlit-1.26.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.10.11-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pinecone-client\n",
            "  Downloading pinecone_client-2.2.2-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.1/179.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Collecting pydantic<2.0,>=1.9 (from chromadb)\n",
            "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chroma-hnswlib==0.7.2 (from chromadb)\n",
            "  Downloading chroma-hnswlib-0.7.2.tar.gz (31 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain)\n",
            "  Downloading langsmith-0.0.32-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.8.0)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<10,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Collecting pympler<2,>=0.9 (from streamlit)\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.5.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting tzlocal<5,>=1.1 (from streamlit)\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.21.2-py3-none-any.whl (25 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.33-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.6/188.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8 (from streamlit)\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.2)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loguru>=0.5.0 (from pinecone-client)\n",
            "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython>=2.0.0 (from pinecone-client)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.4)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.16.2)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
            "Collecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit)\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.9.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tzdata (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.3)\n",
            "Building wheels for collected packages: chroma-hnswlib, sentence_transformers, pypika\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.2-cp310-cp310-linux_x86_64.whl size=2287479 sha256=64b2f97ba402eddc808ccada6ae6e1142a9a8eebaef5839a428a5da511ac706f\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/2b/0d/ee457f6782f75315bb5828d5c2dc5639d471afbd44a830b9dc\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=013bf3652e4a402c24df106df60c933a0749c8463abac0b0ddb3859da30fa647\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=350035050aa64aa1761eeaa22ee712cc3567eaa916128497e8cd1ca9056a775f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built chroma-hnswlib sentence_transformers pypika\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, pypika, monotonic, filetype, websockets, watchdog, validators, uvloop, tzdata, smmap, python-magic, python-dotenv, pypdf, pympler, pydantic, pulsar-client, overrides, mypy-extensions, marshmallow, loguru, humanfriendly, httptools, h11, emoji, dnspython, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, pytz-deprecation-shim, pydeck, posthog, pinecone-client, langsmith, huggingface-hub, gitdb, coloredlogs, tzlocal, transformers, openai, onnxruntime, gitpython, fastapi, dataclasses-json, unstructured, langchain, chromadb, streamlit, sentence_transformers\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.2.1\n",
            "    Uninstalling pydantic-2.2.1:\n",
            "      Successfully uninstalled pydantic-2.2.1\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.0.1\n",
            "    Uninstalling tzlocal-5.0.1:\n",
            "      Successfully uninstalled tzlocal-5.0.1\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.0.1 chroma-hnswlib-0.7.2 chromadb-0.4.8 coloredlogs-15.0.1 dataclasses-json-0.5.14 dnspython-2.4.2 emoji-2.8.0 fastapi-0.99.1 filetype-1.2.0 gitdb-4.0.10 gitpython-3.1.33 h11-0.14.0 httptools-0.6.0 huggingface-hub-0.16.4 humanfriendly-10.0 langchain-0.0.278 langsmith-0.0.32 loguru-0.7.0 marshmallow-3.20.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.15.1 openai-0.28.0 overrides-7.4.0 pinecone-client-2.2.2 posthog-3.0.2 pulsar-client-3.3.0 pydantic-1.10.12 pydeck-0.8.0 pympler-1.0.1 pypdf-3.15.4 pypika-0.48.9 python-dotenv-1.0.0 python-magic-0.4.27 pytz-deprecation-shim-0.1.0.post0 safetensors-0.3.3 sentence_transformers-2.2.2 sentencepiece-0.1.99 smmap-5.0.0 starlette-0.27.0 streamlit-1.26.0 tiktoken-0.4.0 tokenizers-0.13.3 transformers-4.32.1 typing-inspect-0.9.0 tzdata-2023.3 tzlocal-4.3.1 unstructured-0.10.11 uvicorn-0.23.2 uvloop-0.17.0 validators-0.21.2 watchdog-3.0.0 watchfiles-0.20.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# open-source and free vector store\n",
        "# https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
        "# https://python.langchain.com/docs/integrations/vectorstores/\n",
        "!pip install chromadb\n",
        "import chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXTUTRjFacYs",
        "outputId": "ba2f0ecd-ebce-455f-9abf-25ee6c3c0615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.8)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.12)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.2)\n",
            "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.99.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.15.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.13.3)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.cache import InMemoryCache\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.vectorstores import  Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "# If we have a large document we need to split it\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "import tiktoken\n",
        "import langchain\n",
        "import os"
      ],
      "metadata": {
        "id": "jXSdFXHFjl08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Document --> split into chunks\n",
        "\n",
        "# use embedding model --> embed chunks --> vectors\n",
        "\n",
        "# vector chunks --> save chromaDB\n",
        "\n",
        "# \"query\" -->"
      ],
      "metadata": {
        "id": "rzTLHysBY0oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/api_key.txt\")\n",
        "api_key = f.read()\n",
        "model = ChatOpenAI(openai_api_key=api_key, temperature = 0)\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "QbkhLPqpV6Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Pdf"
      ],
      "metadata": {
        "id": "gZKx5EdGfyIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loader = PyPDFLoader(\"/content/SomeReport.pdf\")\n",
        "#documents = loader.load()\n",
        "# data[0].page_content.replace(\"\\n\",\" \")\n",
        "# replacing \\n with a space\n",
        "#text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500)\n",
        "#docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "71qgfH0CVQdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for txt"
      ],
      "metadata": {
        "id": "fz1eouUtf0En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(\"/content/About_DT_FInal.txt\")\n",
        "documents = loader.load()\n",
        "# data[0].page_content.replace(\"\\n\",\" \")\n",
        "# replacing \\n with a space\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "6kfCP_yvfxNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_function = OpenAIEmbeddings()\n",
        "\n",
        "# We're going to  use Chroma to pass both the embeddings and the documentation into its database.\n",
        "db = Chroma.from_documents(docs, embedding_function, persist_directory=\"./gau_db\")\n",
        "\n",
        "# DB persist in order to force the save to your actual computer or whateveryou're working with.\n",
        "db.persist()\n",
        "\n",
        "# Now, after we shut down our computer and we want to reload our vector\n",
        "db_new_connection = Chroma(persist_directory = '/content/gau_db',\n",
        "                           embedding_function=embedding_function)"
      ],
      "metadata": {
        "id": "HusfMklPVQk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inside Chroma --> text str / (1,2,3)\n",
        "# new str --> [3,4,5] --> similarity search in chroma"
      ],
      "metadata": {
        "id": "SZpVvLsKcAQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_doc = \"Wishkarma\""
      ],
      "metadata": {
        "id": "NkfrYVt2cAUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_docs = db_new_connection.similarity_search(new_doc)"
      ],
      "metadata": {
        "id": "LnfKlY4DfU05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(similar_docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSBQvN0Rh8Pu",
        "outputId": "edbcddb3-6c3b-453a-e45e-8ef1ff6b88a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(similar_docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmVQxzldfU31",
        "outputId": "0d3f4f63-d6b0-4b89-89be-490178c2a317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wishkarma \n",
            "During the period of 13th to 18th March, the team members at Wishkarma made significant contributions to various projects, resulting in improved website functionality, user experience, and content creation. Here is a detailed summary of their accomplishments:\n",
            "1. Soubhik Banerjee:\n",
            "   * Implemented changes to the side navigation bar, including making it vertical, adding hover effects, and search functionality.\n",
            "   * Improved responsiveness and styling of the website, enhancing the overall user experience.\n",
            "2. Ahmed Zhran:\n",
            "   * Handled the notification system backend, improving the functionality and user experience of the notification system.\n",
            "   * Worked on the referral feature, sitemap generator, and admin-dashboard project initialization.\n",
            "3. Sabeena Viklar:\n",
            "   * Created a chatbot, enhancing website usability and increasing efficiency for users.\n",
            "   * Improved the overall user experience and functionality of the website through the creation of the chatbot.\n",
            "4. Krishna Shenoy:\n",
            "   * Created a user management dashboard frontend page, improving user management and ease of use on the website.\n",
            "   * Integrated user management backend services and explored chatbot functionalities.\n",
            "5. Disha Mishra:\n",
            "   * Worked on Spacekit Lite, contributing to its development and writing two blogs.\n",
            "   * Created content for Spacekit Nano, including three blogs, and edited previously posted blogs.\n",
            "   * Contributed to content posting on Pinterest, increasing the reach and engagement of Wishkarma's content.\n",
            "6. Sri Ganesh Nazre:\n",
            "   * Made various contributions, including creating Pinterest posts for MOA, blog and Pinterest images for Spacekit NANO, and preparing blog images for MOA.\n",
            "   * Wrote a blog on \"A Modern Narrative Of Tradition\" and published it, improving website content and user experience.\n",
            "   * Updated the Whatsapp CTA in My MOA Blogs, enhancing user engagement.\n",
            "   * Contributed to the overall accuracy and presentation of product information, positively impacting Wishkarma's marketing and branding efforts.\n",
            "7. Arpita Rathod:\n",
            "   * Worked on creating structures for newsletters, improving email marketing strategies.\n",
            "   * Contributed to blog writing and email copywriting, enhancing the overall content creation process.\n",
            "8. During the period of 6th to 11th March, Soubhik Banerjee continued his contributions by updating the navbar, addressing responsiveness issues, and making changes to the products page on mobile devices.\n",
            "9. Sri Ganesh Nazre made further contributions, including writing and publishing a blog, creating and publishing a YouTube short, attending a meeting with an SEO consultant, and working on images for a new blog. These efforts improved website content and SEO optimization.\n",
            "10. Krishna Shenoy accomplished tasks such as scraping a website for potential customer contacts, resolving UI issues, creating new endpoints for the admin portal, understanding and prioritizing new requirements, and learning about authorization in web-based solutions.\n",
            "11. Chhavi Naidu played a role in building brand awareness through blog writing, publishing blogs on WordPress for SEO purposes, and sending out 20 emails. These efforts aimed to improve website traffic, engagement, and user loyalty.\n",
            "12. Sabeena Viklar solved an issue related to image display on the website, learned new concepts, and attended a socratic dialogue session. These contributions improved website functionality and her own knowledge in areas like automated email sending and competitive programming.\n",
            "13. Arpita Rathod conducted research to find unique email templates, assisted a teammate in solving a problem, and attended a review session. These efforts contributed to improving email marketing strategies and collaboration within the team.\n",
            "Overall, the team members' contributions at Wishkarma resulted in improved website functionality, user experience, content creation, and marketing efforts. Their work positively impacted various aspects of the organization, including navigation, notifications,.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data connection process"
      ],
      "metadata": {
        "id": "Yb6O92JajZy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.cache import InMemoryCache\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.vectorstores import  Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "# If we have a large document we need to split it\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "VBBH9BPbcAdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/content/SomeReport.pdf\")\n",
        "documents = loader.load()\n",
        "# data[0].page_content.replace(\"\\n\",\" \")\n",
        "# replacing \\n with a space\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(\"Translate into hindi:\\n{comment}\")\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
        "result = model(chat_prompt.format_prompt(comment = docs[0].page_content).to_messages())\n",
        "\n",
        "result.content.replace(\"\\n\",\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PXK3RjW9WOBe",
        "outputId": "89b6115b-ea93-495c-d198-e721f11883dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'यह पहली लाइन पीडीएफ़ है। यह दूसरी लाइन पीडीएफ़ में है। यह तीसरी लाइन पीडीएफ़ में है।'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting the external data and generating the output"
      ],
      "metadata": {
        "id": "y0wp1F76SXvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jsmies_template = \" List out only the names of interns from the text:'\\n\\{clients}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(jsmies_template)"
      ],
      "metadata": {
        "id": "KvnkcwTjShNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.messages[0].prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_-S3dCXShPm",
        "outputId": "0a443ed6-3a1b-4a92-f60a-98c5d887dd51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['clients'], output_parser=None, partial_variables={}, template=\" List out only the names of interns from the text:'\\n\\\\{clients}.\", template_format='f-string', validate_template=True)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jsmies_messages = prompt_template.format_messages(\n",
        "    clients = similar_docs[0].page_content\n",
        ")"
      ],
      "metadata": {
        "id": "ed5RVDPvShSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jsmies_response = model(jsmies_messages)"
      ],
      "metadata": {
        "id": "OlO5mAtuShUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jsmies_response.content.split(\",\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB1tysqQWjCB",
        "outputId": "350161cf-c82b-4433-a678-08df4af32870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Soubhik Banerjee',\n",
              " ' Ahmed Zhran',\n",
              " ' Sabeena Viklar',\n",
              " ' Krishna Shenoy',\n",
              " ' Disha Mishra',\n",
              " ' Sri Ganesh Nazre',\n",
              " ' Arpita Rathod',\n",
              " ' Chhavi Naidu']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume Assessor"
      ],
      "metadata": {
        "id": "56Yndj5Ibvf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "crXhA5e6dV_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "GgjKN_0Tc_Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/content/python-developer-resume-example.pdf\")"
      ],
      "metadata": {
        "id": "VGWQPFrkdZ_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = loader.load()"
      ],
      "metadata": {
        "id": "iDcjD_RHdaBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = data2[0].page_content\n",
        "resume_data = text.replace(\"\\n\",\" \")\n",
        "print(resume_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPnlhCmRdjJU",
        "outputId": "f7f068ff-eb5b-40fd-870a-55425d907c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G I U L I A G O N Z A L E Z P y t h o n  D e v e l o p e r ggonzalez@email.com (123) 456-7890 Detroit, MI LinkedIn Github E D U C A T I O N M.S. Computer Science University of Chicago 2014 - 2016 Chicago, IL B.S. Computer Science University of Pittsburgh 2010 - 2014 Pittsburgh, PA S K I L L S HTML/ CSS SQL (PostgreSQL, Oracle) JavaScript (Angular) Python (Django) REST APIs (GraphQL) AWS (Redshift, S3) GitW O R K  E X P E R I E N C E Python Developer DoorDash September 2017 - current Detroit, MI ·Worked on building new Angular components for the customer- facing web app, which improved the time on page for the average user by 2 minutes ·Collaborated with an agile team of 6, and helped prioritize and scope feature requests to ensure that the biggest impact features were worked on ﬁrst ·Built extensive test coverage for all new features, which reduced the number of customer complaints by 23% ·Acquired and ingested data to build and maintain data pipelines that led to discovering an opportunity for a new site feature, boosting revenue by 6% ·Communicated with internal teams and stakeholders, working to determine solutions for the user experience Python Developer Intern Knewton April 2016 - April 2017 Chicago, IL ·Worked alongside another developer to implement RESTful APIs in Django that enabled internal analytics team to increase reporting speed by 24% ·Using Selenium, built out a unit testing infrastructure for a client web application that reduced the number of bugs reported by the client by 11% month over month ·Provided project updates to leadership team of 3, and oﬀered recommendations for design ·Diagnosed issues causing slow speeds in applications, and documented the process to making the database query system more robust ·Participated in writing scalable code with a team of 4 interns and 1 developer for applications for a math course P R O J E C T S Cryptocurrency Price Tracker Creator ·Incorporated API calls to several applications, and stored data eﬃciently in PostgreSQL backend ·Utilized D3.js to allow users to dynamically visualize price movements over time periods of their choosing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "Job_role: Does it match the job description? \\\n",
        "Answer Match if yes, No-match if not or unknown.\n",
        "\n",
        "Applied for: What role has the candidate applied for? \\\n",
        "Mentions the applied role\n",
        "\n",
        "Skills: What Skills are mentioned? \\\n",
        "If this information is not found, output -1.\n",
        "\n",
        "Weakness: Extract any sentences about the weakness regarding the applied role\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "Job_role\n",
        "Applied for\n",
        "Skills\n",
        "Weakness\n",
        "\n",
        "text: {resume}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "o4ogFwfFShXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "prompt_template = ChatPromptTemplate.from_template(resume_template)\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JewpizIkIFH",
        "outputId": "78fb404e-c904-498a-fd30-ac4ad960f3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['resume'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['resume'], output_parser=None, partial_variables={}, template='For the following text, extract the following information:\\n\\nJob_role: Does it match the job description? Answer Match if yes, No-match if not or unknown.\\n\\nApplied for: What role has the candidate applied for? Mentions the applied role\\n\\nSkills: What Skills are mentioned? If this information is not found, output -1.\\n\\nWeakness: Extract any sentences about the weakness regarding the applied roleand output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\nJob_role\\nApplied for\\nSkills\\nWeakness\\n\\ntext: {resume}\\n', template_format='f-string', validate_template=True), additional_kwargs={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = prompt_template.format_messages(resume = resume_data)"
      ],
      "metadata": {
        "id": "5iMjAB1ckM-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(temperature=0.0)\n",
        "response = chat(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm307U1pkwKH",
        "outputId": "d8798259-cf96-4d74-b8c2-c00ec2118c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Job_role\": \"Match\",\n",
            "  \"Applied for\": \"Python Developer\",\n",
            "  \"Skills\": \"HTML/ CSS, SQL (PostgreSQL, Oracle), JavaScript (Angular), Python (Django), REST APIs (GraphQL), AWS (Redshift, S3), Git\",\n",
            "  \"Weakness\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationBufferMemory"
      ],
      "metadata": {
        "id": "ttjYkocvozcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "chNLOJK9o27S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k = 3)\n",
        "conversation = ConversationChain(\n",
        "    llm = model,\n",
        "    memory = memory,\n",
        "    verbose = False\n",
        ")"
      ],
      "metadata": {
        "id": "TELN1uHDo3LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({'input':\"Hi\"}, {\"output\":\"Welcome to JSMIES!'\\n'How may I help you?\"})"
      ],
      "metadata": {
        "id": "kTMVlhx3pYkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7umyq5rpYmY",
        "outputId": "8c5a41ac-11ea-43c8-b880-2d4c2796c475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi\n",
            "AI: Hello! How can I assist you today?\n",
            "Human: Hi\n",
            "AI: Welcome to JSMIES!'\n",
            "'How may I help you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({'input':\"What's your name?\"},\n",
        " {\"output\":\"My name is Samaya.\"})"
      ],
      "metadata": {
        "id": "UU0zqVGrpYpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhXSHGtcpYu5",
        "outputId": "99db3b8a-c2a9-4c07-a5ef-d1411e7396e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi\n",
            "AI: Hello! How can I assist you today?\n",
            "Human: Hi\n",
            "AI: Welcome to JSMIES!'\n",
            "'How may I help you?\n",
            "Human: What's your name?\n",
            "AI: My name is Samaya.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"Hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hjMGAV3cpYww",
        "outputId": "68cfa47d-7062-44c4-8049-2d9b82ae5ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ConversataionTokenBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationTokenBufferMemory"
      ],
      "metadata": {
        "id": "4G2FNg3MpYyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(\n",
        "    llm = model,\n",
        "    max_token_limit= 30\n",
        ")"
      ],
      "metadata": {
        "id": "yO3cbJ-XpY07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({'input':\"Hi\"}, {\"output\":\"Welcome to JSMIES!'\\n'How may I help you?\"})\n",
        "memory.save_context({'input':\"What's your name?\"},\n",
        " {\"output\":\"I'm Samaya.\"})\n"
      ],
      "metadata": {
        "id": "b7sk7ikGthXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGJuX66qpY3K",
        "outputId": "02fbbeaf-e30f-4c9a-8eaf-74a34fc6b801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: What's your name?\n",
            "AI: I'm Samaya.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QA using Vector Database"
      ],
      "metadata": {
        "id": "lYIKMegyz2_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain[docarray]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYNgVpjnWxZx",
        "outputId": "79c25772-1c77-40e9-9a58-ede0969419f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain[docarray] in /usr/local/lib/python3.10/dist-packages (0.0.270)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (0.0.25)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (8.2.3)\n",
            "Collecting docarray[hnswlib]<0.33.0,>=0.32.0 (from langchain[docarray])\n",
            "  Downloading docarray-0.32.1-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain[docarray]) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain[docarray]) (0.9.0)\n",
            "Collecting orjson>=3.8.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])\n",
            "  Downloading orjson-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (13.5.2)\n",
            "Collecting types-requests>=2.28.11.6 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])\n",
            "  Downloading types_requests-2.31.0.2-py3-none-any.whl (14 kB)\n",
            "Collecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain[docarray]) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[docarray]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[docarray]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[docarray]) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain[docarray]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain[docarray]) (23.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (2.16.1)\n",
            "Collecting types-urllib3 (from types-requests>=2.28.11.6->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])\n",
            "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain[docarray]) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (0.1.2)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2202689 sha256=b50eb1ceed8b04923e941f1ff67e29a86766cc2ca203e9f4c3b7f2edf411c41e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: types-urllib3, types-requests, orjson, hnswlib, docarray\n",
            "Successfully installed docarray-0.32.1 hnswlib-0.7.0 orjson-3.9.5 types-requests-2.31.0.2 types-urllib3-1.26.25.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from IPython.display import display, Markdown\n",
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "K1UeNONRo3cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(\"/content/About DT FInal.txt\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "KXTeRnCCVziQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = DocArrayInMemorySearch.from_documents(\n",
        "    docs,\n",
        "    embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "XkTSTxJVWqM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "nahFAjJFXVmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(openai_api_key = api_key, temperature=0)\n",
        "qa_stuff = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "OIcW0kZ7VzkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is DeepThought?\""
      ],
      "metadata": {
        "id": "p92d2jN3VznQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = qa_stuff.run(question)"
      ],
      "metadata": {
        "id": "LCj2Nc6hVzpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "UCMTFOvoVztH",
        "outputId": "e8d924ed-ac63-4a82-d99f-7e8f84f148da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "DeepThought is an organization that focuses on nurturing thought leaders who possess the ability to tackle real-life problems through interdisciplinary and cross-functional thinking. They value innovative thinking and creative inductive leaps, recognizing that breakthroughs often arise from zero-to-one thinking. DeepThought is committed to creating socio-economic value and instills a culture of rigor, robustness, and research. They prioritize user-centric problem-solving and embrace diversity and inclusion. DeepThought goes beyond conventional boundaries and explores new frontiers in ethics and aesthetics. They also play a role in redefining the education industry by grooming young individuals to contribute to disruptive product startups. Overall, DeepThought aims to create a world where critical thinking, innovation, and the development of future leaders drive positive change in society and the economy."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dt_qa(query):\n",
        "\n",
        "  docs = loader.load()\n",
        "  db = DocArrayInMemorySearch.from_documents(\n",
        "      docs,\n",
        "      embeddings\n",
        "  )\n",
        "  retriever = db.as_retriever()\n",
        "\n",
        "  llm = ChatOpenAI(openai_api_key = api_key, temperature=0)\n",
        "  qa_stuff = RetrievalQA.from_chain_type(\n",
        "      llm=llm,\n",
        "      chain_type=\"stuff\",\n",
        "      retriever=retriever,\n",
        "      verbose=False\n",
        "  )\n",
        "\n",
        "  response = qa_stuff.run(query)\n",
        "\n",
        "  return display(Markdown(response))"
      ],
      "metadata": {
        "id": "k1_BinBHYcVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_qa(\"Generate the summary on the performance of growpital in 50 words for UI.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "us4MWQimZz81",
        "outputId": "dc75c009-106b-4603-bf7e-0da353b89975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Growpital's UI team has shown remarkable leadership and expertise in MERN stack development, testing, research, and marketing. They have implemented 12 major features, integrated 12 APIs, and developed 20 UI components, significantly improving user experience. Their creative development extends beyond coding, as they have designed sequence diagrams and added functionalities to the admin panel."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With memory"
      ],
      "metadata": {
        "id": "TklJD7XI566x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain[docarray] pypdf chromadb langchain tiktoken openai faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89SXK7pj5-NS",
        "outputId": "f2a6324b-60cc-4c03-b5c7-b92be2658624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Collecting langchain[docarray]',\n",
              " '  Downloading langchain-0.0.271-py3-none-any.whl (1.5 MB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/1.5 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.1/1.5 MB\\x1b[0m \\x1b[31m2.0 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.4/1.5 MB\\x1b[0m \\x1b[31m5.3 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.7/1.5 MB\\x1b[0m \\x1b[31m6.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━\\x1b[0m \\x1b[32m1.1/1.5 MB\\x1b[0m \\x1b[31m7.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━\\x1b[0m \\x1b[32m1.5/1.5 MB\\x1b[0m \\x1b[31m8.3 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m1.5/1.5 MB\\x1b[0m \\x1b[31m7.9 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting pypdf',\n",
              " '  Downloading pypdf-3.15.2-py3-none-any.whl (271 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/271.1 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m271.1/271.1 kB\\x1b[0m \\x1b[31m10.3 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting chromadb',\n",
              " '  Downloading chromadb-0.4.7-py3-none-any.whl (415 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/415.5 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m \\x1b[32m409.6/415.5 kB\\x1b[0m \\x1b[31m16.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m415.5/415.5 kB\\x1b[0m \\x1b[31m11.3 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting tiktoken',\n",
              " '  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/1.7 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.6/1.7 MB\\x1b[0m \\x1b[31m17.8 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━\\x1b[0m \\x1b[32m1.3/1.7 MB\\x1b[0m \\x1b[31m18.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m1.7/1.7 MB\\x1b[0m \\x1b[31m18.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m1.7/1.7 MB\\x1b[0m \\x1b[31m15.5 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting openai',\n",
              " '  Downloading openai-0.27.9-py3-none-any.whl (75 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/75.5 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m75.5/75.5 kB\\x1b[0m \\x1b[31m8.7 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hRequirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)',\n",
              " 'Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (6.0.1)',\n",
              " 'Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (2.0.20)',\n",
              " 'Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (3.8.5)',\n",
              " 'Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (4.0.3)',\n",
              " 'Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain[docarray])',\n",
              " '  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)',\n",
              " 'Collecting langsmith<0.1.0,>=0.0.21 (from langchain[docarray])',\n",
              " '  Downloading langsmith-0.0.26-py3-none-any.whl (34 kB)',\n",
              " 'Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (2.8.5)',\n",
              " 'Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (1.23.5)',\n",
              " 'Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (2.2.0)',\n",
              " 'Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (2.31.0)',\n",
              " 'Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (8.2.3)',\n",
              " 'Collecting docarray[hnswlib]<0.33.0,>=0.32.0 (from langchain[docarray])',\n",
              " '  Downloading docarray-0.32.1-py3-none-any.whl (215 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/215.3 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m215.3/215.3 kB\\x1b[0m \\x1b[31m18.1 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting pydantic<3,>=1 (from langchain[docarray])',\n",
              " '  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/3.1 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.9/3.1 MB\\x1b[0m \\x1b[31m28.9 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m1.8/3.1 MB\\x1b[0m \\x1b[31m26.3 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━\\x1b[0m \\x1b[32m2.9/3.1 MB\\x1b[0m \\x1b[31m27.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m3.1/3.1 MB\\x1b[0m \\x1b[31m27.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m3.1/3.1 MB\\x1b[0m \\x1b[31m21.2 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting chroma-hnswlib==0.7.2 (from chromadb)',\n",
              " '  Downloading chroma-hnswlib-0.7.2.tar.gz (31 kB)',\n",
              " '  Installing build dependencies ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Getting requirements to build wheel ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Preparing metadata (pyproject.toml) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " 'Collecting fastapi<0.100.0,>=0.95.2 (from chromadb)',\n",
              " '  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/58.4 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m58.4/58.4 kB\\x1b[0m \\x1b[31m5.5 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)',\n",
              " '  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/59.5 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m59.5/59.5 kB\\x1b[0m \\x1b[31m5.7 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting posthog>=2.4.0 (from chromadb)',\n",
              " '  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)',\n",
              " 'Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)',\n",
              " 'Collecting pulsar-client>=3.1.0 (from chromadb)',\n",
              " '  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/5.3 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m1.4/5.3 MB\\x1b[0m \\x1b[31m43.6 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m2.9/5.3 MB\\x1b[0m \\x1b[31m42.3 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━\\x1b[0m \\x1b[32m4.7/5.3 MB\\x1b[0m \\x1b[31m44.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m5.3/5.3 MB\\x1b[0m \\x1b[31m44.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m5.3/5.3 MB\\x1b[0m \\x1b[31m33.3 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)',\n",
              " '  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/5.9 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m1.9/5.9 MB\\x1b[0m \\x1b[31m60.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━\\x1b[0m \\x1b[32m4.4/5.9 MB\\x1b[0m \\x1b[31m62.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m5.9/5.9 MB\\x1b[0m \\x1b[31m64.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m5.9/5.9 MB\\x1b[0m \\x1b[31m45.9 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting tokenizers>=0.13.2 (from chromadb)',\n",
              " '  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/7.8 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m2.0/7.8 MB\\x1b[0m \\x1b[31m70.6 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m4.5/7.8 MB\\x1b[0m \\x1b[31m67.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m7.8/7.8 MB\\x1b[0m \\x1b[31m82.4 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m7.8/7.8 MB\\x1b[0m \\x1b[31m82.4 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m7.8/7.8 MB\\x1b[0m \\x1b[31m44.3 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting pypika>=0.48.9 (from chromadb)',\n",
              " '  Downloading PyPika-0.48.9.tar.gz (67 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/67.3 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m67.3/67.3 kB\\x1b[0m \\x1b[31m6.4 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25h  Installing build dependencies ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Getting requirements to build wheel ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Preparing metadata (pyproject.toml) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " 'Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)',\n",
              " 'Collecting overrides>=7.3.1 (from chromadb)',\n",
              " '  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)',\n",
              " 'Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)',\n",
              " 'Collecting bcrypt>=4.0.1 (from chromadb)',\n",
              " '  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/593.7 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m593.7/593.7 kB\\x1b[0m \\x1b[31m48.0 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)',\n",
              " 'Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (23.1.0)',\n",
              " 'Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (3.2.0)',\n",
              " 'Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (6.0.4)',\n",
              " 'Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.9.2)',\n",
              " 'Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.4.0)',\n",
              " 'Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.3.1)',\n",
              " 'Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain[docarray])',\n",
              " '  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/49.4 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m49.4/49.4 kB\\x1b[0m \\x1b[31m5.2 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain[docarray])',\n",
              " '  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)',\n",
              " 'Collecting orjson>=3.8.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])',\n",
              " '  Downloading orjson-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/139.9 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m139.9/139.9 kB\\x1b[0m \\x1b[31m12.7 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hRequirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (13.5.2)',\n",
              " 'Collecting types-requests>=2.28.11.6 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])',\n",
              " '  Downloading types_requests-2.31.0.2-py3-none-any.whl (14 kB)',\n",
              " 'Collecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])',\n",
              " '  Downloading hnswlib-0.7.0.tar.gz (33 kB)',\n",
              " '  Installing build dependencies ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Getting requirements to build wheel ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Preparing metadata (pyproject.toml) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " 'Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (3.20.3)',\n",
              " 'Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb)',\n",
              " '  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/67.0 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m67.0/67.0 kB\\x1b[0m \\x1b[31m7.0 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)',\n",
              " '  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/46.0 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m46.0/46.0 kB\\x1b[0m \\x1b[31m4.9 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)',\n",
              " 'Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)',\n",
              " 'Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)',\n",
              " 'Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)',\n",
              " 'Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)',\n",
              " '  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)',\n",
              " 'Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)',\n",
              " '  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)',\n",
              " 'Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)',\n",
              " 'Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)',\n",
              " 'Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[docarray]) (3.4)',\n",
              " 'Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[docarray]) (2.0.4)',\n",
              " 'Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain[docarray]) (2.0.2)',\n",
              " 'Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7)',\n",
              " 'Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)',\n",
              " '  Downloading h11-0.14.0-py3-none-any.whl (58 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/58.3 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m58.3/58.3 kB\\x1b[0m \\x1b[31m6.7 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)',\n",
              " '  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/428.8 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m428.8/428.8 kB\\x1b[0m \\x1b[31m29.4 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)',\n",
              " '  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)',\n",
              " 'Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)',\n",
              " '  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/4.1 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━\\x1b[0m \\x1b[32m3.6/4.1 MB\\x1b[0m \\x1b[31m108.3 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m4.1/4.1 MB\\x1b[0m \\x1b[31m66.3 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)',\n",
              " '  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/1.3 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m1.3/1.3 MB\\x1b[0m \\x1b[31m62.7 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)',\n",
              " '  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/129.9 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m129.9/129.9 kB\\x1b[0m \\x1b[31m12.0 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (3.0.0)',\n",
              " 'Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (2.16.1)',\n",
              " 'Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)',\n",
              " 'Collecting types-urllib3 (from types-requests>=2.28.11.6->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])',\n",
              " '  Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)',\n",
              " 'Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain[docarray])',\n",
              " '  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)',\n",
              " 'Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)',\n",
              " '  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/86.8 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m86.8/86.8 kB\\x1b[0m \\x1b[31m7.8 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)',\n",
              " 'Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)',\n",
              " 'Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.3)',\n",
              " 'Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (0.1.2)',\n",
              " 'Building wheels for collected packages: chroma-hnswlib, pypika, hnswlib',\n",
              " '  Building wheel for chroma-hnswlib (pyproject.toml) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.2-cp310-cp310-linux_x86_64.whl size=2287474 sha256=8f1e6e885cf50801879a8cb5ca179232167ea4d4813b3399d1090831f3fb271c',\n",
              " '  Stored in directory: /root/.cache/pip/wheels/11/2b/0d/ee457f6782f75315bb5828d5c2dc5639d471afbd44a830b9dc',\n",
              " '  Building wheel for pypika (pyproject.toml) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=b8a478db7c2ad6fb88646ea4a22adb4b013e69397c9d47443cc472984b3485ee',\n",
              " '  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4',\n",
              " '  Building wheel for hnswlib (pyproject.toml) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2202683 sha256=70c16324f5a9451aeaf3886a4310f7a07ed0714db1f90abd6af9d05ccc3c6e3a',\n",
              " '  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4',\n",
              " 'Successfully built chroma-hnswlib pypika hnswlib',\n",
              " 'Installing collected packages: types-urllib3, tokenizers, pypika, monotonic, websockets, uvloop, types-requests, python-dotenv, pypdf, pydantic, pulsar-client, overrides, orjson, mypy-extensions, marshmallow, humanfriendly, httptools, hnswlib, h11, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, langsmith, coloredlogs, openai, onnxruntime, fastapi, docarray, dataclasses-json, langchain, chromadb',\n",
              " '  Attempting uninstall: pydantic',\n",
              " '    Found existing installation: pydantic 2.2.0',\n",
              " '    Uninstalling pydantic-2.2.0:',\n",
              " '      Successfully uninstalled pydantic-2.2.0',\n",
              " 'Successfully installed backoff-2.2.1 bcrypt-4.0.1 chroma-hnswlib-0.7.2 chromadb-0.4.7 coloredlogs-15.0.1 dataclasses-json-0.5.14 docarray-0.32.1 fastapi-0.99.1 h11-0.14.0 hnswlib-0.7.0 httptools-0.6.0 humanfriendly-10.0 langchain-0.0.271 langsmith-0.0.26 marshmallow-3.20.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.15.1 openai-0.27.9 orjson-3.9.5 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.2.0 pydantic-1.10.12 pypdf-3.15.2 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 tiktoken-0.4.0 tokenizers-0.13.3 types-requests-2.31.0.2 types-urllib3-1.26.25.14 typing-inspect-0.9.0 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "# Creating retrival tool\n",
        "from langchain.agents.agent_toolkits import create_retriever_tool\n",
        "# Agent Constructor\n",
        "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from IPython.display import display, Markdown\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "# Caching\n",
        "# https://python.langchain.com/docs/modules/model_io/models/llms/llm_caching\n",
        "from langchain.cache import InMemoryCache, SQLiteCache\n",
        "from langchain.document_loaders.text import TextLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "# ConversationTokenBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationTokenBufferMemory, ConversationBufferMemory\n",
        "import langchain\n",
        "import os\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "zvp7o-9g5-Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = open(\"/content/api_key.txt\").read()\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "llm = ChatOpenAI(openai_api_key= api_key, temperature=0)"
      ],
      "metadata": {
        "id": "B-cTey2_5-iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dt_qa(query):\n",
        "\n",
        "  # dependencies\n",
        "  langchain.llm_cache = SQLiteCache(database_path= \"langchain.db\")\n",
        "\n",
        "  embeddings = OpenAIEmbeddings()\n",
        "  loader = TextLoader(\"/content/About DT FInal.txt\")\n",
        "  documents = loader.load()\n",
        "  text_splitter = CharacterTextSplitter(chunk_size=2000,\n",
        "                                        chunk_overlap=0)\n",
        "  texts = text_splitter.split_documents(documents)\n",
        "  embeddings = OpenAIEmbeddings()\n",
        "  db = FAISS.from_documents(texts,\n",
        "                            embeddings)\n",
        "\n",
        "  retriever = db.as_retriever()\n",
        "\n",
        "  tool = create_retriever_tool(\n",
        "      retriever,\n",
        "      \"search_About_DT_FInal\",\n",
        "      \"Searches and returns documents regarding the About_DT_FInal.\"\n",
        "  )\n",
        "  tools = [tool]\n",
        "\n",
        "  agent_executor = create_conversational_retrieval_agent(llm,\n",
        "                                                         tools,\n",
        "                                                         verbose=False)\n",
        "\n",
        "  result = agent_executor({\"input\": query})\n",
        "\n",
        "  return display(Markdown(result[\"output\"]))"
      ],
      "metadata": {
        "id": "medvZwiO5-3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbGmJRsZ5_Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gf1CGlZWEqUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JkaszB8hEqkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qRbjjaZqEqvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JPV1pKftEq63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.llm_cache = SQLiteCache(database_path= \"langchain.db\")\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "loader = TextLoader(\"/content/About DT FInal.txt\")\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=2000,\n",
        "                                      chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = FAISS.from_documents(texts,\n",
        "                          embeddings)\n",
        "\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "hZwusIbDErGT",
        "outputId": "eed8ea1c-5ba5-406f-f5c9-ccb59cf62df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-22ecd1cc2ab8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLiteCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"langchain.db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/About DT FInal.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SQLiteCache' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"search_About_DT_FInal\",\n",
        "    \"Searches and returns documents regarding the About_DT_FInal.\"\n",
        ")\n",
        "tools = [tool]\n",
        "\n",
        "agent_executor = create_conversational_retrieval_agent(llm,\n",
        "                                                        tools,\n",
        "                                                        verbose=False)"
      ],
      "metadata": {
        "id": "o4xtMh1T5_NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent_executor({\"input\": \"What is DeepThought?\"})"
      ],
      "metadata": {
        "id": "gNoPumxI5_Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(result[\"output\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "Z_AqgTSI5_jp",
        "outputId": "c316ae6b-f5fe-4c14-f3c9-59b49010d5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "DeepThought is an organization that nurtures thought leaders who possess the ability to tackle real-life problems through interdisciplinary and cross-functional thinking. They are committed to creating socio-economic value and instilling a culture of rigor, robustness, and research. DeepThought celebrates innovative thinking and creative inductive leaps, fostering an environment where moments of innovation are cherished and encouraged. They prioritize user-centricity in problem-solving and value a broad range of perspectives. DeepThought also explores new frontiers in ethics and aesthetics and actively promotes collaborative questioning and creative thinking through Socratic Dialogue competitions. Their overarching vision is to create a world where everyone is a researcher, driving positive change in society and the economy through critical thinking, innovation, and the development of future leaders."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent_executor({\"input\": \"What is at its core?\"})"
      ],
      "metadata": {
        "id": "aHOxLLbMFJtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(result[\"output\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "_pjjKPdFFOMi",
        "outputId": "aab28940-5335-40f8-95fd-0f4bda219d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "At its core, DeepThought values and celebrates innovative thinking and creative inductive leaps. They recognize that true breakthroughs often arise from zero-to-one thinking and actively foster an environment where such moments of innovation are cherished and encouraged. DeepThought also emphasizes the importance of interdisciplinary and cross-functional thinking, as well as critical analysis of assumptions and the process of unlearning and relearning. They strive for consistent evolution and growth, even in the face of challenges, and are driven by their belief in the power of diversity and inclusion. User-centricity is another core principle of DeepThought, as they prioritize understanding the needs and desires of their users and develop solutions that truly resonate with them."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Components"
      ],
      "metadata": {
        "id": "mtlh7Jf2IByM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is needed for both the memory and the prompt\n",
        "memory_key = \"history\""
      ],
      "metadata": {
        "id": "Ui0kJjieH7JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
        "\n",
        "memory = AgentTokenBufferMemory(memory_key=memory_key, llm=llm)"
      ],
      "metadata": {
        "id": "G33i-n4OH7Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
        "from langchain.schema.messages import SystemMessage\n",
        "from langchain.prompts import MessagesPlaceholder"
      ],
      "metadata": {
        "id": "tEtBnCAXH7dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = SystemMessage(\n",
        "        content=(\n",
        "            \"Do your best to answer the questions. \"\n",
        "            \"Feel free to use any tools available to look up \"\n",
        "            \"relevant information, only if neccessary\"\n",
        "        )\n",
        ")"
      ],
      "metadata": {
        "id": "LCHnOOP-H7qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = OpenAIFunctionsAgent.create_prompt(\n",
        "        system_message=system_message,\n",
        "        extra_prompt_messages=[MessagesPlaceholder(variable_name=memory_key)]\n",
        "    )"
      ],
      "metadata": {
        "id": "e0TGxxo8H73V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)"
      ],
      "metadata": {
        "id": "WugSuSTKH8Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor"
      ],
      "metadata": {
        "id": "BEz1mPgqH8ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True,\n",
        "                                   return_intermediate_steps=True)"
      ],
      "metadata": {
        "id": "iDAyeXFEKQCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent_executor({\"input\": \"hi, im bob\"})"
      ],
      "metadata": {
        "id": "hVpRj0i_KQPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent_executor({\"input\": \"whats my name\"})"
      ],
      "metadata": {
        "id": "cnYwz6uyKQd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pAFTeUpPKQue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_rT3vNbH8o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "262xYRNimKER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eEb7n6wlmKGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JRNeEzskmKJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.schema import Document\n",
        "import pinecone\n",
        "from pypdf import PdfReader\n",
        "from langchain.llms.openai import OpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHR-pDS6mKNT",
        "outputId": "bc0f79aa-23b6-4a0b-8696-5fa6b6dd4ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RQxv39uanyRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract Information from PDF file\n",
        "def get_pdf_text(pdf_doc):\n",
        "    text = \"\"\n",
        "    pdf_reader = PdfReader(pdf_doc)\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# iterate over files in\n",
        "# that user uploaded PDF files, one by one\n",
        "def create_docs(user_pdf_list, unique_id):\n",
        "    docs=[]\n",
        "    for filename in user_pdf_list:\n",
        "\n",
        "        chunks=get_pdf_text(filename)\n",
        "\n",
        "        #Adding items to our list - Adding data & its metadata\n",
        "        docs.append(Document(\n",
        "            page_content=chunks,\n",
        "            metadata={\"name\": filename.name,\"id\":filename.id,\"type=\":filename.type,\"size\":filename.size,\"unique_id\":unique_id},\n",
        "        ))\n",
        "\n",
        "    return docs\n",
        "\n",
        "\n",
        "#Create embeddings instance\n",
        "def create_embeddings_load_data():\n",
        "    #embeddings = OpenAIEmbeddings()\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "#Function to push data to Vector Store - Pinecone here\n",
        "def push_to_pinecone(pinecone_apikey,pinecone_environment,pinecone_index_name,embeddings,docs):\n",
        "\n",
        "    pinecone.init(\n",
        "    api_key=pinecone_apikey,\n",
        "    environment=pinecone_environment\n",
        "    )\n",
        "    print(\"done......2\")\n",
        "    Pinecone.from_documents(docs, embeddings, index_name=pinecone_index_name)\n",
        "\n",
        "\n",
        "\n",
        "#Function to pull infrmation from Vector Store - Pinecone here\n",
        "def pull_from_pinecone(pinecone_apikey,pinecone_environment,pinecone_index_name,embeddings):\n",
        "\n",
        "    pinecone.init(\n",
        "    api_key=pinecone_apikey,\n",
        "    environment=pinecone_environment\n",
        "    )\n",
        "\n",
        "    index_name = pinecone_index_name\n",
        "\n",
        "    index = Pinecone.from_existing_index(index_name, embeddings)\n",
        "    return index\n",
        "\n",
        "\n",
        "\n",
        "#Function to help us get relavant documents from vector store - based on user input\n",
        "def similar_docs(query,k,pinecone_apikey,pinecone_environment,pinecone_index_name,embeddings,unique_id):\n",
        "\n",
        "    pinecone.init(\n",
        "    api_key=pinecone_apikey,\n",
        "    environment=pinecone_environment\n",
        "    )\n",
        "\n",
        "    index_name = pinecone_index_name\n",
        "\n",
        "    index = pull_from_pinecone(pinecone_apikey,pinecone_environment,index_name,embeddings)\n",
        "    similar_docs = index.similarity_search_with_score(query, int(k),{\"unique_id\":unique_id})\n",
        "    #print(similar_docs)\n",
        "    return similar_docs\n",
        "\n",
        "\n",
        "# Helps us get the summary of a document\n",
        "def get_summary(current_doc):\n",
        "    llm = OpenAI(temperature=0)\n",
        "    #llm = HuggingFaceHub(repo_id=\"bigscience/bloom\", model_kwargs={\"temperature\":1e-10})\n",
        "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "    summary = chain.run([current_doc])\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "j-hTpGM3mKPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc(\"/content/software-engineer-iii-front-end-resume-example.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "kFjR3SDln4hr",
        "outputId": "2f4fc0e5-7e41-4a23-e62c-ac8d1bdef903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-60fa0a5c9731>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/software-engineer-iii-front-end-resume-example.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
          ]
        }
      ]
    }
  ]
}